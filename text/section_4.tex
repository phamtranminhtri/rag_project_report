\section{Retrieval-Augmented Generation for Large Language Models: A Survey}

Link to the paper: \url{https://arxiv.org/abs/2312.10997} \cite{gao2023retrieval}

\subsection{Overview of RAG}

Retrieval-Augmented Generation (RAG) is a paradigm that enhances large language models (LLMs) by incorporating external knowledge retrieved at inference time. Instead of relying solely on parametric knowledge stored in model weights, RAG dynamically retrieves relevant documents and uses them as additional context for response generation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{image/4_01.png}
    \caption{RAG overview}
    % \label{fig:enter-label}
\end{figure}


\subsubsection{Naive RAG}

The Naive RAG framework follows a simple \textit{Retrieve--Read} pipeline consisting of three main stages: indexing, retrieval, and generation. During indexing, raw data sources such as PDFs, HTML pages, or Word documents are converted into plain text, segmented into smaller chunks, and encoded into vector representations stored in a vector database. In the retrieval stage, the user query is embedded and compared with stored vectors using similarity metrics to obtain the top-$K$ most relevant chunks. Finally, in the generation stage, the LLM produces an answer based on the user query and the retrieved context, optionally incorporating conversation history.

Despite its simplicity, Naive RAG suffers from several limitations. Retrieval may lack precision and recall, resulting in irrelevant or missing information. During generation, the model may hallucinate content not supported by the retrieved context or produce outputs with bias or irrelevance. Moreover, effectively integrating retrieved information across different tasks remains challenging, often leading to redundant, incoherent, or overly extractive responses.

\subsubsection{Advanced RAG}

Advanced RAG aims to improve retrieval quality and context utilization through enhanced pre-retrieval and post-retrieval techniques. Pre-retrieval optimization focuses on improving indexing structures and query formulation, including data granularity control, metadata alignment, mixed retrieval strategies, and query rewriting or expansion. Post-retrieval optimization emphasizes effective context integration, such as re-ranking retrieved documents to prioritize relevance and compressing context to reduce noise and prompt length.

\subsubsection{Modular RAG}

Modular RAG extends beyond fixed retrieval-generation pipelines by introducing specialized, interchangeable modules. These include search modules for heterogeneous data sources, RAG-Fusion for multi-query expansion and re-ranking, and memory modules that maintain a continuously updated retrieval memory pool. Additional components such as routing, prediction, and task adapters allow RAG systems to dynamically select retrieval pathways and adapt to downstream tasks.

This modular design enables flexible retrieval patterns, including Rewrite--Retrieve--Read, Generate--Read, and hybrid retrieval strategies that combine keyword-based, semantic, and vector-based search. As a result, Modular RAG exhibits strong adaptability and scalability across diverse applications.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{image/4_02.png}
    \caption{Types of RAG}
    % \label{fig:enter-label}
\end{figure}

\subsection{Retrieval}

Retrieval is a core component of RAG, responsible for identifying and supplying relevant external knowledge to the generation model. The effectiveness of a RAG system heavily depends on retrieval source selection, indexing strategies, query optimization, and embedding quality.

\subsubsection{Retrieval Sources and Granularity}

Retrieval sources can be categorized into unstructured, semi-structured, and structured data. Unstructured text data, such as Wikipedia articles or domain-specific documents, is the most common source. Semi-structured data, including PDFs with tables, presents challenges due to structural complexity, while structured sources like knowledge graphs offer precise and verified information at the cost of higher construction and maintenance effort.

Retrieval granularity ranges from tokens and sentences to chunks and full documents. Coarse-grained retrieval provides richer context but may introduce redundancy and noise, whereas fine-grained retrieval improves precision but risks losing essential semantic information.

\subsubsection{Indexing Optimization}

Indexing optimization techniques aim to balance context richness and efficiency. Chunking strategies play a critical role, where large chunks capture broader context but increase noise and computational cost, while small chunks reduce noise but may lack sufficient information. The \textit{Small-to-Big} approach mitigates this trade-off by retrieving smaller units and expanding context hierarchically.

Metadata attachments, such as page numbers or timestamps, enable filtered retrieval and scoped search. Structural indexing methods, including hierarchical document structures and knowledge graph indices, further enhance retrieval speed and relevance. Techniques like Reverse HyDE leverage LLMs to generate potential questions that each chunk can answer, improving retrievability.

\subsubsection{Query Optimization}

Query optimization improves retrieval effectiveness by refining or expanding user queries. Query expansion and multi-query techniques enrich the query with additional context, while sub-query decomposition breaks complex questions into simpler ones. Query transformation methods include rewriting queries, generating hypothetical answers (HyDE), and step-back prompting to retrieve higher-level contextual information.

Query routing mechanisms further enhance retrieval by directing queries to appropriate data sources or pipelines using metadata-based or semantic routing strategies.

\subsubsection{Embeddings and Adapters}

Modern RAG systems often employ hybrid retrieval that combines sparse retrievers, such as BM25 for keyword matching, with dense retrievers based on neural embeddings for semantic understanding. Embedding models can be fine-tuned for domain-specific tasks, with LM-supervised retrievers aligning retrieval objectives with generation outcomes using LLM feedback.

When fine-tuning is impractical, adapter-based methods provide lightweight alternatives. These include prompt retrievers, bridging modules that transform retrieved content into LLM-friendly formats, and plug-in knowledge generators that replace or augment traditional retrievers in white-box settings.

\subsection{Generation}

Generation is the final stage in a Retrieval-Augmented Generation (RAG) pipeline, where the Large Language Model (LLM) produces the final response based on the user query and the augmented context retrieved from external knowledge sources. This stage directly determines the quality, coherence, and usefulness of the system’s output.

\subsubsection{Overview of the Generation Stage}

In the basic RAG setting, also referred to as \textit{Naive RAG}, the generation stage receives the user query together with the retrieved documents as input. These elements are combined into a single prompt, which is then passed to a frozen LLM to generate the final answer.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/generation_naive_rag.png}
    \caption{Generation stage in Naive RAG, where retrieved context is directly passed to a frozen LLM via a prompt}
    \label{fig:generation_naive}
\end{figure}

The generated response is expected to be fluent, context-grounded, and aligned with the retrieved evidence. However, this approach may suffer from noisy or redundant context, which can degrade answer quality.

\subsubsection{Context Curation for Improved Generation}

To address the limitations of Naive RAG, advanced RAG systems introduce a \textit{context curation} step before generation. The goal of context curation is to improve relevance and reduce noise in the retrieved context supplied to the LLM.

Key components of context curation include:
\begin{itemize}
    \item \textbf{Reranking}: reorders retrieved documents to prioritize the most relevant ones.
    \item \textbf{Context Selection or Compression}: filters, summarizes, or shortens retrieved content to avoid overly long prompts.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/generation_context_curation.png}
    \caption{Advanced RAG with post-retrieval context curation before generation}
    \label{fig:generation_context}
\end{figure}

By providing a more concise and focused augmented prompt, context curation enables the LLM to generate higher-quality and more accurate responses.

\subsubsection{LLM Fine-tuning for Generation}

Beyond improving the quality of the input context, generation performance can be further enhanced by fine-tuning the LLM itself. The objective of LLM fine-tuning is to adapt the model to domain-specific knowledge and desired response styles.

The key benefits of LLM fine-tuning include:
\begin{itemize}
    \item \textbf{Domain Adaptation}: improves understanding and reasoning in specialized domains.
    \item \textbf{Output Alignment}: aligns tone, structure, and formatting with predefined guidelines.
    \item \textbf{Reduced Hallucination}: encourages stronger reliance on retrieved evidence.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/generation_llm_finetuning.png}
    \caption{Generation stage using a fine-tuned LLM in an Advanced RAG pipeline}
    \label{fig:generation_finetune}
\end{figure}

As a result, fine-tuned generation produces responses that are more accurate, coherent, and context-grounded, making it suitable for domain-specific and high-stakes applications.

\subsection{Augmentation}

Augmentation is a key process in Retrieval-Augmented Generation (RAG) systems that enhances answer quality by iteratively refining retrieval and generation. Instead of performing retrieval only once, the system can retrieve additional information, reformulate queries, or adjust context dynamically during the answering process.

\subsubsection{Overview of the Augmentation Process}

In the augmentation process, retrieval and generation are tightly coupled in an iterative loop. After each generation step, the system evaluates whether the current information is sufficient or if additional retrieval is required before producing the final response.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/augmentation_overview.png}
    \caption{Overview of the iterative augmentation process in RAG}
    \label{fig:augmentation_overview}
\end{figure}

The main purposes of augmentation are:
\begin{itemize}
    \item Handling complex or multi-step reasoning questions.
    \item Producing more accurate and context-grounded answers.
\end{itemize}

\subsubsection{Iterative Retrieval}

Iterative retrieval repeatedly alternates between retrieving new context and generating partial answers. Each generation step provides new signals that guide the next retrieval, gradually enriching the available context.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/augmentation_iterative.png}
    \caption{Iterative retrieval with repeated retrieval--generation cycles}
    \label{fig:augmentation_iterative}
\end{figure}

The key ideas behind iterative retrieval include:
\begin{itemize}
    \item Retrieval is performed again based on what has been generated so far.
    \item Context is progressively enriched to support step-by-step reasoning.
\end{itemize}

\textbf{Pros and Cons:}
\begin{itemize}
    \item \textbf{Advantages}: more comprehensive and targeted information.
    \item \textbf{Disadvantages}: risk of semantic drift or accumulation of irrelevant context.
\end{itemize}

\subsubsection{Recursive Retrieval}

Recursive retrieval refines the query step-by-step using feedback from previous retrieval results. Each iteration clarifies what information is still missing, leading to progressively more relevant context.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/augmentation_recursive.png}
    \caption{Recursive retrieval with query transformation and decomposition}
    \label{fig:augmentation_recursive}
\end{figure}

Key characteristics of recursive retrieval include:
\begin{itemize}
    \item Construction of intermediate reasoning structures such as chain-of-thought or clarification trees.
    \item Continuous reformulation of the query to target more specific information.
\end{itemize}

\textbf{Pros and Cons:}
\begin{itemize}
    \item \textbf{Advantages}: improved accuracy and relevance over multiple iterations.
    \item \textbf{Disadvantages}: increased latency due to multiple refinement steps.
\end{itemize}

\subsubsection{Adaptive Retrieval}

Adaptive retrieval allows the LLM to dynamically decide when and what to retrieve during generation. Instead of enforcing retrieval at fixed stages, the model monitors its own confidence and triggers retrieval only when additional context is required.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/augmentation_adaptive.png}
    \caption{Adaptive retrieval where the LLM actively controls retrieval decisions}
    \label{fig:augmentation_adaptive}
\end{figure}

The key ideas of adaptive retrieval are:
\begin{itemize}
    \item Retrieval is performed on demand rather than at predefined stages.
    \item The LLM behaves like an agent, evaluating intermediate outputs and deciding whether to retrieve more context or proceed to the final answer.
\end{itemize}

\subsubsection{Summary}

In summary, augmentation enhances the RAG pipeline by enabling iterative and adaptive interaction between retrieval and generation. Iterative, recursive, and adaptive retrieval strategies provide increasing levels of flexibility and reasoning capability, allowing RAG systems to better handle complex queries and produce more accurate, context-aware responses.

\subsection{Tasks and Evaluation Framework}

\subsubsection{Downstream Tasks and Benchmarks}
The primary objective of RAG systems is to support various Natural Language Processing (NLP) tasks by providing external context. While Question Answering (QA) remains the most prominent application, the scope of RAG is rapidly expanding.

\begin{itemize}
    \item \textbf{Question Answering (QA):} This is the core application, encompassing several complexities:
    \begin{itemize}
        \item \textit{Single-hop QA:} Requires retrieving information from a single document (e.g., \textbf{Natural Questions (NQ), TriviaQA, SQuAD}).
        \item \textit{Multi-hop QA:} Requires synthesizing information across multiple documents to answer complex queries (e.g., \textbf{HotpotQA, 2WikiMultiHopQA, MuSiQue}).
        \item \textit{Domain-specific QA:} Tailored for specialized fields like medicine or research (e.g., \textbf{Qasper, COVID-QA, MMCU Medical}).
    \end{itemize}
    \item \textbf{Expanded Tasks:} Beyond QA, RAG is utilized in \textbf{Information Extraction (IE)}, \textbf{Dialogue Generation}, and \textbf{Code Search}, where external documentation is vital for accuracy.
\end{itemize}

\subsubsection{Evolution of Evaluation Targets}
Traditional evaluation relied on lexical overlap metrics, but modern RAG research shifts toward component-level analysis.

\begin{enumerate}
    \item \textbf{Traditional Metrics:} Metrics such as \textbf{EM (Exact Match)}, \textbf{F1 score}, and \textbf{ROUGE} were standard. However, they fail to evaluate the internal reasoning of the model or the quality of the retrieved context.
    \item \textbf{Modern RAG Metrics:} Evaluation is now bifurcated into two primary targets:
    \begin{itemize}
        \item \textbf{Retrieval Quality:} Measured by \textbf{Hit Rate}, \textbf{Mean Reciprocal Rank (MRR)}, and \textbf{Normalized Discounted Cumulative Gain (NDCG)}.
        \item \textbf{Generation Quality:} Focused on \textbf{Faithfulness} (factuality), \textbf{Relevance} (answering the intent), and \textbf{Non-harmfulness}.
    \end{itemize}
\end{enumerate}

\subsubsection{Evaluation Aspects: The "RAG Triad" and Key Abilities}
Current research emphasizes three quality scores (often referred to as the RAG Triad) and four essential robustness abilities.

\textbf{The Three Quality Scores:}
\begin{itemize}
    \item \textbf{Context Relevance:} Evaluates whether the retrieved snippets are precise and specific to the query, minimizing "noise" or extraneous content.
    \item \textbf{Answer Faithfulness:} Ensures the generated output is derived strictly from the retrieved context without "hallucinating" external or contradictory information.
    \item \textbf{Answer Relevance:} Measures how well the response directly addresses the user's question.
\end{itemize}

\textbf{Four Essential Abilities:}
\begin{itemize}
    \item \textbf{Information Integration:} The capacity to synthesize insights from multiple disparate documents.
    \item \textbf{Noise Robustness:} The ability to filter out irrelevant or misleading information within the retrieved set.
    \item \textbf{Counterfactual Robustness:} The ability to recognize and disregard known inaccuracies or "fake news" injected into documents.
    \item \textbf{Negative Rejection:} The system's ability to decline answering when the provided documents do not contain the necessary knowledge.
\end{itemize}

\subsection{Discussion}

\begin{itemize}
    \item \textbf{RAG vs. Long-Context LLMs}: With modern LLMs supporting contexts over 200,000 tokens, the necessity of RAG is often debated. However, RAG remains superior due to:
    \begin{itemize}
        \item \textbf{Efficiency:} Processing massive contexts per request is computationally expensive and slow. RAG's chunk-based retrieval is significantly faster and more cost-effective.
        \item \textbf{Observability:} RAG provides clear citations and references, whereas Long-Context generation operates as a "black box," making verification difficult.
    \end{itemize}

    \item \textbf{The Noise Paradox}: Recent studies have identified a "Robustness Paradox": in certain scenarios, including a small amount of irrelevant (noisy) documents can unexpectedly improve model accuracy by over 30\%, suggesting that some noise may act as a regularizer during the reasoning process.

    \item \textbf{Hybrid Approaches}: The most effective current strategy is a hybrid model: using \textbf{Fine-tuning} to adapt the model’s behavior and style, while using \textbf{RAG} to provide updated, dynamic knowledge.

    \item \textbf{Scaling Laws}: While LLMs follow predictable scaling laws, it is unclear if these apply to RAG. Some research suggests an \textbf{"Inverse Scaling Law"}, where smaller, specialized models may outperform larger general-purpose models when equipped with high-quality RAG pipelines.

\end{itemize}