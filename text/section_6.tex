\section{Implementation: LangChain, Text-to-speech, Speech-to-text}

\subsection{Building an AI Agent Chatbot with LangChain}

We implement AI agent chatbot using the LangChain framework, designed to support a music-related website through Retrieval-Augmented Generation (RAG) combined with speech-based interaction. LangChain enables the construction of agentic systems by integrating large language models (LLMs) with external tools, memory, and retrieval mechanisms, allowing the chatbot to reason, decide actions, and iteratively solve user queries.

An \textbf{AI agent} in LangChain is defined as a system that combines a language model with a set of tools, enabling it to select and invoke appropriate tools based on the task context. Tools act as functional interfaces that extend the model’s capabilities beyond pure text generation, such as searching documents or querying the web.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\linewidth]{image/6_01.png}
    \caption{AI Agent}
    % \label{fig:enter-label}
\end{figure}



\subsubsection{RAG Agent Architecture}

The chatbot is built around a RAG agent architecture consisting of three main stages: indexing, retrieval, and response generation. During the indexing phase, nearly 100 Wikipedia articles related to {Music} are loaded using LangChain’s \texttt{WikipediaLoader}. These documents are embedded using the \texttt{gemini-embedding-001} model and stored persistently in a \texttt{Chroma} vector database, enabling efficient semantic similarity search.

\subsubsection{Agent Tools}

The agent is equipped with multiple tools to enhance its reasoning and information access:
\begin{itemize}
    \item \textbf{Context Retrieval Tool}: Retrieves the top-$k$ (with $k=10$) most relevant documents from the Chroma vector store based on semantic similarity.
    \item \textbf{Web Search Tool}: Uses DuckDuckGo to fetch real-time search results, including URLs, titles, and snippets, enabling access to up-to-date information.
    \item \textbf{Web Loader Tool}: Employs LangChain’s \texttt{WebBaseLoader} to extract and process raw HTML content from selected web pages.
\end{itemize}

These tools are wrapped and exposed to the agent, allowing it to dynamically decide whether to rely on internal knowledge, retrieved documents, or external web sources.

\subsubsection{Agent Configuration and Memory}

The agent is initialized with a system prompt that defines its role and behavior. The conversational backbone uses the \texttt{gemini-2.5-flash} chat model for response generation. To maintain contextual coherence, short-term memory is incorporated, enabling the agent to remember and reference previous turns within a single conversation thread.

\subsection{Text-to-Speech}

Text-to-Speech (TTS) is a technology that converts written text into synthesized speech, enabling AI systems to produce audible responses for users. In conversational AI and Retrieval-Augmented Generation (RAG) systems, TTS plays an important role in enhancing user interaction by delivering information in a natural and accessible audio format.

\subsubsection{Text-to-Speech Pipeline}

A typical Text-to-Speech system follows a multi-stage processing pipeline that transforms raw text into an audio signal. The main stages of this pipeline are illustrated in Figure~\ref{fig:tts_pipeline}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{image/tts_pipeline.png}
    \caption{General Text-to-Speech pipeline from text input to synthesized speech output}
    \label{fig:tts_pipeline}
\end{figure}

The process begins with \textbf{text pre-processing}, where the input text is normalized by handling numbers, abbreviations, and formatting. Next, \textbf{linguistic analysis} examines grammatical structure and syntactic features. The \textbf{grapheme-to-phoneme conversion} stage maps characters to phonemes to determine correct pronunciation. Based on this phonetic representation, \textbf{pattern determination} defines prosodic features such as rhythm, pitch, and stress. Finally, \textbf{speech signal generation} synthesizes the waveform that produces the audible speech output.

\subsubsection{gTTS: Google Text-to-Speech}

gTTS (Google Text-to-Speech) is a lightweight Python library that provides an easy-to-use interface for converting text into speech. It leverages Google Translate’s text-to-speech service to generate spoken audio from textual input and outputs the result as an MP3 file.

The simplicity of gTTS makes it suitable for rapid prototyping and integration into AI applications where basic speech output is required. Figure~\ref{fig:gtts_example} shows a minimal example of using gTTS to synthesize Vietnamese speech from text.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{image/gtts_example.png}
    \caption{Example of using gTTS in Python to generate an MP3 file from text}
    \label{fig:gtts_example}
\end{figure}

In this example, the input text is passed to the gTTS library along with the target language code. The generated speech is then saved as an audio file, which can be played back by the system. While gTTS is easy to use and freely accessible, it provides limited control over voice characteristics and requires an internet connection.

Overall, Text-to-Speech systems such as gTTS complement language models by enabling multimodal interaction, allowing AI-driven applications to communicate information not only through text but also through natural-sounding speech.

\subsection{Speech-to-text}

Automatic Speech Recognition (ASR), commonly known as Speech-to-Text (STT), is the process of converting spoken language into written text. In this study, two distinct approaches were evaluated: the \textbf{SpeechRecognition} library (utilizing Google Web Speech API) and OpenAI's \textbf{Whisper} model.

\subsubsection{SpeechRecognition with Google Web Speech API}
The \texttt{SpeechRecognition} library serves as a flexible wrapper for several high-level STT APIs. For this implementation, the \textbf{Google Web Speech API} was utilized.

\begin{itemize}
    \item \textbf{Mechanism:} The process follows a client-server architecture. The client records or loads an audio file (typically in \texttt{.wav} format), which is then transmitted to Google’s cloud servers. The server processes the audio using proprietary deep learning models and returns the transcribed text in a JSON format.
    \item \textbf{Technical Characteristics:} Since the heavy computation is handled server-side, the local machine requires minimal processing power. It supports a vast array of languages, including high-accuracy recognition for Vietnamese.
    \item \textbf{Strengths:} Extremely lightweight for the host device, high inference speed, and minimal setup requirements.
\end{itemize}

\subsubsection{OpenAI Whisper}
Whisper is a state-of-the-art, multi-purpose speech recognition model trained on 680,000 hours of multilingual and multitask supervised data. Unlike API-based solutions, Whisper is a \textbf{Transformer-based encoder-decoder} model designed for local deployment.

\begin{itemize}
    \item \textbf{Architecture:}
    \begin{itemize}
        \item \textbf{Input:} The raw audio is first converted into a \textbf{Log-Mel Spectrogram}, which represents the frequency content of the audio over time.
        \item \textbf{Encoder:} A series of convolutional layers and Transformer blocks encode the audio features into a latent representation.
        \item \textbf{Decoder:} An autoregressive Transformer decoder predicts the next text token based on the previously generated tokens and the encoder's audio features.
    \end{itemize}
    \item \textbf{Strengths:} Excellent understanding of context, automatic punctuation, and the ability to function entirely offline.
\end{itemize}

\subsubsection{Experimental Comparison and Evaluation}
Through practical implementation in a demonstration application, both technologies were compared based on performance, resource consumption, and output quality.

\begin{table}[h]
\centering
\caption{Comparison: SpeechRecognition API vs. Whisper Model}
\begin{tabular}{@{}lll@{}}
\textbf{Feature} & \textbf{SpeechRecognition (Google)} & \textbf{OpenAI Whisper} \\ 
\textbf{Inference Speed} & Extremely Fast (Cloud-dependent) & Slower (Hardware-dependent) \\
\textbf{Hardware Req.} & Minimal (Low CPU/RAM) & High (Requires GPU/Strong CPU) \\
\textbf{Connectivity} & Requires Internet & Offline capable \\
\textbf{Formatting} & Basic (No punctuation/casing) & Advanced (Punctuation/Casing) \\
\textbf{Vietnamese} & Excellent recognition & Good, but prone to hallucinations \\
\textbf{Customization} & None (Closed API) & Support for Fine-tuning \\
\end{tabular}
\end{table}

\textbf{Pros and Cons Analysis:}
\begin{itemize}
    \item \textbf{SpeechRecognition:}
    \begin{itemize}
        \item \textit{Pros:} Ideal for low-end hardware; exceptional Vietnamese recognition with zero local training.
        \item \textit{Cons:} Strictly requires an internet connection; limited by request quotas; lacks advanced text formatting in the free tier.
    \end{itemize}
    \item \textbf{Whisper:}
    \begin{itemize}
        \item \textit{Pros:} Provides human-like transcriptions with proper capitalization and grammar; provides privacy through local processing.
        \item \textit{Cons:} Computationally "heavy"; may suffer from \textit{hallucinations} (generating text when there is only background noise).
    \end{itemize}
\end{itemize}

\subsubsection{Conclusion for Prototype Implementation}
Based on the experimental results, \textbf{SpeechRecognition (Google Web Speech API)} was selected for the final demonstration. This decision was driven by its \textbf{ease of integration}, \textbf{high inference speed on low-resource hardware}, and \textbf{superior Vietnamese recognition} without the need for complex local environment configurations. While Whisper offers higher contextual quality, its hardware demands outweigh its benefits for this specific lightweight application demo.