\section{Prerequisite: ANN, Transformer, LLM}

\subsection{Artificial Neural Network}

Artificial Neural Networks (ANNs) are computational models inspired by the biological neural system. An ANN consists of multiple artificial neurons interconnected through weighted connections, enabling the model to learn complex mappings between input data and output predictions. Neural networks are widely used in classification, regression, and pattern recognition tasks due to their strong representation capability.

\subsubsection{Overall Architecture of Neural Networks}

A typical feedforward neural network is composed of three main components: an input layer, one or more hidden layers, and an output layer. Information flows from the input layer through hidden layers to produce the final output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{image/network_structure.png}
    \caption{Basic architecture of a feedforward neural network}
    \label{fig:nn_structure}
\end{figure}

Each neuron in one layer is usually connected to all neurons in the next layer through weighted edges, allowing the network to model complex relationships.

\subsubsection{Mathematical Model of an Artificial Neuron}

An artificial neuron performs a weighted summation of its inputs followed by a nonlinear transformation. Given $n$ input features $x_i$, the neuron computes an intermediate value $z$ as:

\begin{equation}
z = \sum_{i=1}^{n} w_i x_i + b
\end{equation}

where $w_i$ denotes the weight associated with input $x_i$, and $b$ is the bias term. The output of the neuron is obtained by applying an activation function $f(\cdot)$:

\begin{equation}
y = f(z)
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{image/neuron_model.png}
    \caption{Mathematical structure of an artificial neuron}
    \label{fig:neuron_model}
\end{figure}

\subsubsection{Activation Functions}

Activation functions introduce non-linearity into the neural network. Without them, the network would behave as a linear model and fail to capture complex patterns in data. Common activation functions include:

\begin{itemize}
    \item \textbf{Sigmoid}: maps input values into the range $(0,1)$, often used in binary classification.
    \item \textbf{Tanh}: outputs values in the range $(-1,1)$, providing zero-centered activations.
    \item \textbf{ReLU (Rectified Linear Unit)}: defined as $f(z) = \max(0, z)$, widely used due to its simplicity and effectiveness.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{image/activation_functions.png}
    \caption{Common activation functions: Sigmoid, Tanh, and ReLU}
    \label{fig:activation}
\end{figure}

\subsubsection{Single-layer and Multi-layer Perceptrons}

A \textbf{single-layer perceptron} contains no hidden layers and can only solve linearly separable problems. In contrast, a \textbf{multi-layer perceptron (MLP)} consists of one or more hidden layers, enabling the network to learn complex nonlinear relationships.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{image/perceptron_comparison.png}
    \caption{Comparison between single-layer and multi-layer perceptrons}
    \label{fig:perceptron}
\end{figure}

\subsubsection{Forward Propagation}

Forward propagation is the process in which input data is passed through the network layer by layer to compute the output prediction. At each layer, weighted sums and activation functions are applied to transform the data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{image/forward_propagation.png}
    \caption{Forward propagation through hidden layers}
    \label{fig:forward}
\end{figure}

\subsubsection{Loss Function}

The loss function measures the discrepancy between the predicted output $\hat{y}$ and the true target value $y$. It acts as a guidance signal for the learning process, where a smaller loss indicates better model performance. Common loss functions include:

\begin{itemize}
    \item Mean Squared Error (MSE) for regression tasks:
    \begin{equation}
        L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    \end{equation}
    \item Cross-Entropy Loss for classification tasks:
    \begin{equation}
        L = - \sum_{i=1}^{n} y_i \log(\hat{y}_i)
    \end{equation}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{image/loss_function.png}
    \caption{Illustration of prediction error and loss minimization}
    \label{fig:loss}
\end{figure}

\subsubsection{Backpropagation and Weight Update}

Backpropagation is an algorithm used to compute the gradient of the loss function with respect to each weight in the network. Based on these gradients, weights are updated iteratively using optimization methods such as gradient descent.

The training process of a neural network consists of three repeated steps:
\begin{enumerate}
    \item Forward propagation
    \item Backpropagation
    \item Weight update
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{image/backpropagation.png}
    \caption{Training process: forward propagation, backpropagation, and weight update}
    \label{fig:backprop}
\end{figure}

\subsection{Transformer architecture}

\subsubsection{Introduction and Background}
Prior to the advent of the Transformer, sequence modeling and transduction problems, such as machine translation and language modeling, were dominated by Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs). However, these architectures faced several inherent limitations:
\begin{itemize}
    \item \textbf{Sequential Computation:} They require $O(n)$ steps to process a sequence of length $n$, preventing parallelization during training.
    \item \textbf{Vanishing and Exploding Gradients:} Despite improvements in LSTMs, these models still struggle with gradient stability in very long sequences.
    \item \textbf{Long-range Dependencies:} Difficulty in capturing relationships between distant tokens in a sentence.
    \item \textbf{Efficiency Decay:} Performance significantly drops as the sequence length increases.
\end{itemize}

In 2015, the Attention mechanism was introduced as an enhancement for Seq2seq models. By 2017, the landmark paper \textit{"Attention is All You Need"} proposed the Transformer architecture, which entirely replaced recurrence with attention. The Transformer follows an \textbf{Encoder-Decoder} structure:
\begin{itemize}
    \item \textbf{Encoder:} Processes the input sequence and generates continuous representations.
    \item \textbf{Decoder:} Generates the output sequence token by token, attending to the encoder's output.
    \item Both components consist of multiple identical layers stacked to increase the model's capacity and representational power.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{image/transformer.png}
    \caption{Transformer architecture}
    \label{fig:transfromer}
\end{figure}

\subsubsection{Self-Attention Mechanism}
The core of the Transformer is the \textbf{Scaled Dot-Product Attention}. Unlike standard attention, Self-Attention relates different positions of a single sequence to compute a representation of the same sequence.

For an input embedding $X$, we compute three vectors: \textbf{Query ($Q$)}, \textbf{Key ($K$)}, and \textbf{Value ($V$)} using learned weight matrices $W^Q, W^K, W^V$:
\begin{equation}
    Q = XW^Q, \quad K = XW^K, \quad V = XW^V
\end{equation}

The attention scores are calculated as follows:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where $d_k$ is the dimension of the keys.

\textbf{Advantages:}
\begin{itemize}
    \item \textbf{Parallelization:} Unlike RNNs, all tokens are processed simultaneously.
    \item \textbf{Global Context:} It captures both local and global dependencies regardless of their distance in the sequence.
    \item \textbf{Training Speed:} When combined with masking, the training process becomes highly efficient.
\end{itemize}

\subsubsection{Multi-Head Attention (MHA)}
Instead of performing a single attention function, Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions.
\begin{equation}
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
\end{equation}
where each $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$.

\textbf{Purpose:}
\begin{itemize}
    \item Enables the model to learn multiple types of relationships (e.g., syntactic vs. semantic) within a single block.
    \item Further enhances parallel computation capabilities.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{image/mha.png}
    \caption{MultiHead Attention}
    \label{fig:mha}
\end{figure}

\subsubsection{Feed-Forward Network and Add-Norm}
Each layer in our encoder and decoder contains a fully connected \textbf{Position-wise Feed-Forward Network (FFN)}:
\begin{equation}
    \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\end{equation}
This consists of two linear transformations with a ReLU activation in between, projecting the data to a higher dimension and then back to stabilize representations.

To ensure deep networks can be trained effectively, each sub-layer (Attention and FFN) is wrapped in an \textbf{Add-Norm} layer:
\begin{itemize}
    \item \textbf{Residual Connection (Add):} Helps avoid the vanishing gradient problem and retains early information.
    \item \textbf{Layer Normalization (Norm):} Normalizes the inputs to stabilize the learning process.
\end{itemize}

\subsubsection{Embeddings and Positional Encoding}
Since the Transformer contains no recurrence or convolution, it has no inherent sense of the relative or absolute position of tokens. 
\begin{itemize}
    \item \textbf{Token Embedding:} Converts text into fixed-size vectors.
    \item \textbf{Positional Encoding:} Unique vectors are added to the input embeddings to provide information about the order of tokens.
    \item \textbf{Shift Right:} In the decoder, the output is shifted to the right to ensure that the prediction for a token only depends on the previously generated tokens (preventing "looking into the future").
\end{itemize}

\subsubsection{Operational Flow}
\textbf{Training Phase:}
\begin{enumerate}
    \item Source and target sequences are embedded and augmented with positional encodings.
    \item The Encoder processes the source sequence through multiple layers. The final output serves as the \textit{Key} and \textit{Value} for the Decoder's \textit{Cross-Attention} mechanism.
    \item The Decoder receives the masked target input, allowing it to compute all positions in parallel via \textit{Masked Multi-Head Attention}.
    \item The final output passes through a Linear layer and a Softmax function to predict tokens, calculating loss for backpropagation.
\end{enumerate}

\textbf{Inference Phase:}
The process is similar but \textbf{autoregressive}. The target sequence starts empty (or with a start token). Each predicted token is appended to the input of the decoder for the next step until an end-of-sequence token is generated.

\subsubsection{Comparative Analysis}
The following table summarizes the differences between traditional recurrent models and the Transformer:

\begin{table}[h]
\centering
\caption{Comparison: LSTM/GRU vs. Transformer}
\begin{tabular}{@{}lll@{}}
\textbf{Feature} & \textbf{LSTM / GRU} & \textbf{Transformer} \\
\textbf{Training} & Slow (Sequential computation) & Fast (Highly parallelizable) \\
\textbf{Inference} & Similar speed to Transformer & Efficient \\
\textbf{Long-range Dependency} & Poor (Memory fades over time) & Excellent (Global attention) \\
\textbf{Short/Medium Sequences} & Performs very well & Performs exceptionally \\
\textbf{Very Long Sequences} & Significant performance drop & Superior (but requires memory optimization) \\
\end{tabular}
\end{table}

\subsection{Large language model}


Large Language Models (LLMs) are advanced neural network models designed to understand, generate, and manipulate natural language at scale. They are typically built upon the Transformer architecture and trained on massive text corpora, enabling them to perform a wide range of Natural Language Processing (NLP) tasks such as text generation, question answering, summarization, and dialogue systems.

\subsubsection{Evolution and Architecture}

The development of LLMs has progressed from traditional statistical language models (e.g., n-gram models) to recurrent architectures such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, and ultimately to Transformer-based models. Transformers address the limitations of sequential computation and long-term dependency learning through the self-attention mechanism, making them highly scalable and effective for large datasets.

Based on their architectural design, LLMs can be categorized into three main types:
\begin{itemize}
    \item \textbf{Decoder-only models (causal language models):} These models, such as GPT and LLaMA, predict the next token in a sequence and are primarily used for text generation tasks.
    \item \textbf{Encoder-only models (bidirectional language models):} Models like BERT, RoBERTa, and DistilBERT focus on learning contextual representations of text and are widely used for text understanding tasks such as classification and semantic similarity.
    \item \textbf{Encoder--Decoder models (sequence-to-sequence):} Examples include T5 and BART, which encode the input sequence and then decode it into an output sequence, making them suitable for machine translation, summarization, and question answering.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{image/2_3_01.png}
    \caption{Transformer architectures}
    % \label{fig:enter-label}
\end{figure}

\subsubsection{Pretraining and Fine-Tuning}

LLMs are first trained during a {pretraining} phase using large-scale datasets such as Common Crawl, Wikipedia, and book corpora. Common pretraining objectives include autoregressive language modeling, masked language modeling, and denoising tasks. These objectives allow the model to learn grammar, semantics, and world knowledge from raw text.

After pretraining, LLMs are adapted to specific tasks through {fine-tuning}. This process may include Supervised Fine-Tuning (SFT), where human-labeled prompt--response pairs are used, and Reinforcement Learning from Human Feedback (RLHF), where human preferences guide the optimization of model outputs. Parameter-Efficient Fine-Tuning (PEFT) techniques such as LoRA and QLoRA are often employed to reduce computational cost while maintaining performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{image/2_3_03.png}
    \caption{RLHF for ChatGPT}
    % \label{fig:enter-label}
\end{figure}



\subsubsection{LLMs in Chatbot Systems}

LLMs play a central role in modern chatbot systems. To improve factual accuracy and domain specificity, Retrieval-Augmented Generation (RAG) is commonly used. In a RAG-based system, relevant documents are embedded into a vector space and stored in a vector database. When a user submits a query, the system retrieves the most relevant documents and provides them as additional context to the LLM before generating a response. This approach significantly enhances the reliability and explainability of chatbot answers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{image/2_3_02.png}
    \caption{Retrieval augmented generation}
    % \label{fig:enter-label}
\end{figure}


In addition, prompt engineering and system instructions are used to control the behavior, style, and scope of the chatbot. For domain-specific applications such as a music information website, these techniques ensure that responses remain relevant, concise, and aligned with predefined constraints.
